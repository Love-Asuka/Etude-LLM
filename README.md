# Etude LLMv0.2

<p align="center">
  <a href="https://huggingface.co/Etude-AI/Etude-LLMv0.2">
    <img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue" alt="Hugging Face">
  </a>
  <a href="LICENSE">
    <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
  </a>
  <a href="https://www.python.org/">
    <img src="https://img.shields.io/badge/Python-3.8%2B-blue" alt="Python">
  </a>
  <a href="https://pytorch.org/">
    <img src="https://img.shields.io/badge/PyTorch-2.0%2B-red" alt="PyTorch">
  </a>
  <a href="https://huggingface.co/transformers/">
    <img src="https://img.shields.io/badge/ü§ó-Transformers-yellow" alt="Transformers">
  </a>
</p>

> **Ê≥®ÊÑè**: Êú¨ÊñáÊ°£‰∏∫Êú∫Âô®ÁøªËØëÁâàÊú¨„ÄÇÂ¶ÇÈúÄÊü•ÁúãÂéüÂßã‰∏≠ÊñáÊñáÊ°£ÔºåËØ∑ËÆøÈóÆ [README_CN.md](README_CN.md)

## Project Overview

Etude LLM is a lightweight language model implementation project designed to provide a customizable and extensible language model architecture. The project adopts a modular design and supports standard Transformer structures. The project name "Etude" (Etude) symbolizes that this project is both a learning practice for language model implementation and can serve as a foundation for more complex model architectures.



![Etude](./img/Etude.gif)

## Core Features

### Model Architecture

Etude LLM implements the following core components:

1. **Attention Mechanism**:
   - Multi-Head Attention (MultiHeadAttention)

2. **Transformer Blocks**:
   - Standard Feed-Forward Network (FeedForward)

### Data Processing

The project provides various data processing tools:

- **XML Processing**: Extract and clean text from XML format data such as Wikipedia
- **Text Splitting**: Split large text files into training samples
- **JSONL Processing**: Process JSON format training data

### Training Methods

Supports multiple training paradigms:

- **Pretraining**: Basic language model training
- **Supervised Fine-Tuning (SFT)**: Instruction fine-tuning training
- **Tokenizer Training**: Custom tokenizer training

## Quick Start

### Training Process Overview

The Etude LLM training process consists of three main steps:

1. **Tokenizer Training** - Create custom tokenizer
2. **Pretraining** - Basic language model training
3. **Supervised Fine-Tuning (SFT)** - Instruction fine-tuning training

### Detailed Training Steps

#### 1. Tokenizer Training

First, train a custom tokenizer:

```bash
cd train
python train_tokenizer.py all
```

This will:
- Validate configuration files
- Train BPE tokenizer
- Create Hugging Face compatible tokenizer configuration
- Validate tokenizer functionality

#### 2. Pretraining

Perform basic language model pretraining:

```bash
cd train
python train_pretrain.py
```

Configuration parameters (can be adjusted in `config.py`):
- Batch size: 16
- Learning rate: 3e-4
- Training epochs: 3
- Device: Auto-detect CUDA/CPU

#### 3. Supervised Fine-Tuning (SFT)

Perform instruction fine-tuning on the pretrained model:

```bash
cd train
python train_sft.py
```

Configuration parameters:
- Batch size: 8
- Learning rate: 3e-5
- Training epochs: 3

### Data Preparation

Training data should be placed in the following directories:

Examples:

- Pretraining data: `training_data/pretrain/pretrain_hq.jsonl`
- SFT data: `training_data/sft/sft_mini_512.jsonl`

### Model Saving

After training completion, models will be saved in:
- Pretrained model: `weight/etude_pretrained_model/`
- SFT model: `weight/etude_sft_model/`
- Tokenizer: `weight/tokenizer/`

### Resume Training

All training scripts support checkpoint resumption:
- Automatically detect checkpoint files
- Restore optimizer state and training progress
- Support continuing SFT training from pretrained models

### Model Inference

After training completion, you can use inference scripts for model inference:

```bash
cd inference
python inference.py
```

This script will load the model directly from the saved model configuration files without relying on training configuration classes.

Additionally, the project provides Hugging Face compatible inference scripts:

```bash
cd inference
python hf.py
```

This script uses the Hugging Face Transformers library to load the model and supports streaming output and chat templates.

### Hugging Face Format Conversion

The project provides a tool script to convert trained Etude models to Hugging Face compatible format:

```bash
cd tool
python convert_hf.py
```

This script will convert Etude model weights and configurations to Llama format, making the model compatible with the Hugging Face ecosystem.

## Technical Features

1. **Modular Design**: Highly decoupled components for easy extension and experimentation
2. **Flexible Configuration**: Unified configuration interface through `EtudeConfig` class

## Use Cases

- Language model research and experimentation
- Text generation applications
- Natural language processing tasks
- Starting point for more complex model development

## Future Development

- Support for more model architecture variants
- Expansion to multimodal tasks
- Inference speed optimization
- Code refactoring

## Summary

- Compared to the previous GPT-2 replication, this version is much better, but still needs significant improvement. Future versions will include code refactoring.
- Although this version can run, the training speed is extremely slow.
- The original v0.1 version could not engage in normal conversation, so it was not preserved.

---

**ÂéüÂßã‰∏≠ÊñáÊñáÊ°£ËØ∑ÂèÇËÄÉÔºö[README_CN.md](README_CN.md)**